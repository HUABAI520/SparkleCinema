spring:
  servlet:
    multipart:
      max-file-size: 1000MB  #上传单个文件的最大大小
      max-request-size: 1200MB #上传文件的总的最大大小
  run:
    jvmArguments: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
  application:
    name: gcsj
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://rm-cn-5yd3gpq2c000hfzo.rwlb.cn-chengdu.rds.aliyuncs.com:3306/hadoop_hdfs?serverTimezone=Asia/Shanghai
    username: root
    password: H123456l
  #redsi 配置
  redis:
    host: 127.0.0.1
    port: 6379
#    password: 123456
    database: 6
  session:
    timeout: 25600
    # 存储到redis中
    store-type: redis
  mvc:
    pathmatch:
      matching-strategy: ant_path_matcher
server:
  address: 0.0.0.0
  port: 8666
  servlet:
    context-path: /hadoop-api
mybatis-plus:
  configuration:
    map-underscore-to-camel-case: false
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
  global-config:
    db-config:
      logic-delete-field: isDelete # 全局逻辑删除的实体字段名
      logic-delete-value: 1 # 逻辑已删除值（默认为 1）
      logic-not-delete-value: 0 # 逻辑未删除值（默认为 0）
spark:
  host: 192.168.189.112 # 集群主节点ip
  hdfsUri: hdfs://192.168.189.112:9000 #hdfs 地址
  user: root  # 集群主节点用户名
  port: 22 # 集群主节点ssh端口
  password: 123456   # 集群主节点ssh密码
  command: "/usr/local/spark/bin/spark-submit /home/python/"  # 集群主节点执行的命令前缀

